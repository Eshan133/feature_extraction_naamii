{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26d5c4e",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4875325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2cea5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ct_path = \"../data/3702_left_knee.nii.gz\"\n",
    "mask_path = \"../data/original_mask.nii.gz\"\n",
    "output_csv = \"../output/similarities.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50bc31",
   "metadata": {},
   "source": [
    "## Step 1: Segmentation-Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f100fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_segment_ct(ct_path, mask_path):\n",
    "    # Load CT scan and mask\n",
    "    ct_img = nib.load(ct_path).get_fdata()\n",
    "    mask_img = nib.load(mask_path).get_fdata()\n",
    "\n",
    "    # Verify mask values\n",
    "    print(f\"Mask Unique Values: {np.unique(mask_img)}\")\n",
    "    \n",
    "    # Account for swapped labels: tibia = 2, femur = 1\n",
    "    tibia_mask = (mask_img == 2).astype(np.float32)  # Tibia (value=2)\n",
    "    femur_mask = (mask_img == 1).astype(np.float32)  # Femur (value=1)\n",
    "    background_mask = (mask_img == 0).astype(np.float32)  # Background\n",
    "\n",
    "    # Apply masks to CT scan\n",
    "    tibia_region = ct_img * tibia_mask\n",
    "    femur_region = ct_img * femur_mask\n",
    "    background_region = ct_img * background_mask\n",
    "\n",
    "    return tibia_region, femur_region, background_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e144661",
   "metadata": {},
   "source": [
    "## Step 2: Convert 2D Pretrained Model to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9dabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_densenet121_to_3d():\n",
    "    # Load pretrained 2D DenseNet121\n",
    "    model_2d = models.densenet121(weights='DEFAULT')  # Use 'weights' for newer PyTorch\n",
    "    model_2d.eval()\n",
    "\n",
    "    # Create a new model for 3D\n",
    "    class DenseNet1213D(nn.Module):\n",
    "        def __init__(self, model_2d):\n",
    "            super(DenseNet1213D, self).__init__()\n",
    "            self.features = nn.Sequential()\n",
    "            for name, module in model_2d.features.named_children():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    # Inflate Conv2d to Conv3d\n",
    "                    out_channels = module.out_channels\n",
    "                    in_channels = module.in_channels\n",
    "                    kernel_size = module.kernel_size[0]  # Assume square kernel\n",
    "                    stride = module.stride[0]\n",
    "                    padding = module.padding[0]\n",
    "                    weight_2d = module.weight.data  # Shape: (out_channels, in_channels, h, w)\n",
    "                    depth = kernel_size  # New depth dimension\n",
    "                    weight_3d = weight_2d.unsqueeze(2).repeat(1, 1, depth, 1, 1) / depth  # Normalize by depth\n",
    "                    conv3d = nn.Conv3d(in_channels, out_channels, \n",
    "                                      kernel_size=(kernel_size, kernel_size, kernel_size),\n",
    "                                      stride=(stride, stride, stride), \n",
    "                                      padding=(padding, padding, padding))\n",
    "                    conv3d.weight.data = weight_3d\n",
    "                    if module.bias is not None:\n",
    "                        conv3d.bias.data = module.bias.data\n",
    "                    self.features.add_module(name, conv3d)\n",
    "                else:\n",
    "                    # Copy non-convolutional layers\n",
    "                    if isinstance(module, nn.MaxPool2d):\n",
    "                        self.features.add_module(name, nn.MaxPool3d(\n",
    "                            kernel_size=module.kernel_size,\n",
    "                            stride=module.stride,\n",
    "                            padding=module.padding))\n",
    "                    elif isinstance(module, nn.BatchNorm2d):\n",
    "                        self.features.add_module(name, nn.BatchNorm3d(module.num_features))\n",
    "                    else:\n",
    "                        self.features.add_module(name, module)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.features(x)\n",
    "\n",
    "    return DenseNet1213D(model_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addb7060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Unique Values: [0. 1. 2.]\n",
      "Segmented regions - Tibia shape: (512, 512, 216), Femur shape: (512, 512, 216), Background shape: (512, 512, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\ACER/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D DenseNet121 model initialized\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough Conv3d layers (1 found, need at least 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m     save_results(similarities, layers, output_csv)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mct_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 97\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(ct_path, mask_path, output_csv)\u001b[0m\n\u001b[0;32m     95\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m features_tibia \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtibia\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m features_femur \u001b[38;5;241m=\u001b[39m extract_features(model_3d, femur, device)\n\u001b[0;32m     99\u001b[0m features_background \u001b[38;5;241m=\u001b[39m extract_features(model_3d, background, device)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(model, volume, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m volume \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(volume)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Shape: (1, 1, D, H, W)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Get convolutional layer names\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m last_layer, third_last_layer, fifth_last_layer \u001b[38;5;241m=\u001b[39m \u001b[43mget_conv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m feature_maps \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Define hook to capture feature maps\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mget_conv_layers\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Ensure we have enough conv layers\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(conv_layers) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough Conv3d layers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(conv_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found, need at least 5)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], conv_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], conv_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Not enough Conv3d layers (1 found, need at least 5)"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Extraction\n",
    "def get_conv_layers(model):\n",
    "    conv_layers = []\n",
    "    for name, module in model.features.named_children():\n",
    "        if isinstance(module, nn.Conv3d):\n",
    "            conv_layers.append(name)\n",
    "    # Ensure we have enough conv layers\n",
    "    if len(conv_layers) < 5:\n",
    "        raise ValueError(f\"Not enough Conv3d layers ({len(conv_layers)} found, need at least 5)\")\n",
    "    return conv_layers[-1], conv_layers[-3], conv_layers[-5]\n",
    "\n",
    "def extract_features(model, volume, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    # Ensure volume is 3D and add batch and channel dimensions\n",
    "    if volume.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D volume, got shape {volume.shape}\")\n",
    "    volume = torch.from_numpy(volume).float().unsqueeze(0).unsqueeze(0).to(device)  # Shape: (1, 1, D, H, W)\n",
    "\n",
    "    # Get convolutional layer names\n",
    "    last_layer, third_last_layer, fifth_last_layer = get_conv_layers(model)\n",
    "    feature_maps = {}\n",
    "\n",
    "    # Define hook to capture feature maps\n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            feature_maps[name] = output\n",
    "        return hook\n",
    "\n",
    "    # Register hooks\n",
    "    for name, module in model.features.named_children():\n",
    "        if name in [last_layer, third_last_layer, fifth_last_layer]:\n",
    "            module.register_forward_hook(hook_fn(name))\n",
    "\n",
    "    # Run model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(volume)\n",
    "\n",
    "    # Apply global average pooling\n",
    "    feature_vectors = {}\n",
    "    for name in feature_maps:\n",
    "        fmap = feature_maps[name].squeeze(0)  # Shape: (C, D, H, W)\n",
    "        gap = nn.AdaptiveAvgPool3d(1)(fmap).squeeze().cpu().numpy()  # Shape: (C,)\n",
    "        feature_vectors[name] = gap\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "# Step 4: Feature Comparison\n",
    "def compute_cosine_similarity(features_tibia, features_femur, features_background):\n",
    "    layers = list(features_tibia.keys())\n",
    "    similarities = {\n",
    "        'Tibia_Femur': [],\n",
    "        'Tibia_Background': [],\n",
    "        'Femur_Background': []\n",
    "    }\n",
    "\n",
    "    for layer in layers:\n",
    "        tibia_vec = features_tibia[layer].reshape(1, -1)\n",
    "        femur_vec = features_femur[layer].reshape(1, -1)\n",
    "        background_vec = features_background[layer].reshape(1, -1)\n",
    "\n",
    "        sim_tibia_femur = cosine_similarity(tibia_vec, femur_vec)[0][0]\n",
    "        sim_tibia_background = cosine_similarity(tibia_vec, background_vec)[0][0]\n",
    "        sim_femur_background = cosine_similarity(femur_vec, background_vec)[0][0]\n",
    "\n",
    "        similarities['Tibia_Femur'].append(sim_tibia_femur)\n",
    "        similarities['Tibia_Background'].append(sim_tibia_background)\n",
    "        similarities['Femur_Background'].append(sim_femur_background)\n",
    "\n",
    "    return similarities, layers\n",
    "\n",
    "# Step 5: Result Organization\n",
    "def save_results(similarities, layers, output_csv):\n",
    "    # Create DataFrame with one row for the image pair\n",
    "    df_data = {}\n",
    "    for i, layer in enumerate(layers):\n",
    "        df_data[f'{layer}_Tibia_Femur'] = [similarities['Tibia_Femur'][i]]\n",
    "        df_data[f'{layer}_Tibia_Background'] = [similarities['Tibia_Background'][i]]\n",
    "        df_data[f'{layer}_Femur_Background'] = [similarities['Femur_Background'][i]]\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "\n",
    "# Main Pipeline\n",
    "def main(ct_path, mask_path, output_csv):\n",
    "    # Step 1: Segment CT scan\n",
    "    tibia, femur, background = load_and_segment_ct(ct_path, mask_path)\n",
    "    print(f\"Segmented regions - Tibia shape: {tibia.shape}, Femur shape: {femur.shape}, Background shape: {background.shape}\")\n",
    "\n",
    "    # Step 2: Initialize 3D model\n",
    "    model_3d = inflate_densenet121_to_3d()\n",
    "    print(\"3D DenseNet121 model initialized\")\n",
    "\n",
    "    # Step 3: Extract features\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    features_tibia = extract_features(model_3d, tibia, device)\n",
    "    features_femur = extract_features(model_3d, femur, device)\n",
    "    features_background = extract_features(model_3d, background, device)\n",
    "    print(\"Features extracted for all regions\")\n",
    "\n",
    "    # Step 4: Compute cosine similarities\n",
    "    similarities, layers = compute_cosine_similarity(features_tibia, features_femur, features_background)\n",
    "    print(f\"Cosine similarities computed for layers: {layers}\")\n",
    "\n",
    "    # Step 5: Save results\n",
    "    save_results(similarities, layers, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(ct_path, mask_path, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
